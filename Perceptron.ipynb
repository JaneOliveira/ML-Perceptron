{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYd1HNhp/PCer3R/DfoaCz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaneOliveira/ML-Perceptron/blob/main/Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# brincando com ML\n",
        "\n",
        "####Criando Perceptron\n",
        "---\n",
        "Sem usar conceitos de orientação a objetos"
      ],
      "metadata": {
        "id": "p1SVob6TptQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def cria_base_dados():\n",
        "\n",
        "    base_dados = np.genfromtxt('base.txt')\n",
        "\n",
        "    return base_dados\n",
        "\n",
        "def cria_conj_teste(base, n_amostras, coluna):\n",
        "    \n",
        "    conj_teste = np.zeros((n_amostras, coluna))\n",
        "    \n",
        "    for i in range(n_amostras):\n",
        "        for j in range(coluna):\n",
        "            conj_teste[i,j] = base[i,j]\n",
        "       \n",
        "    return conj_teste\n",
        "\n",
        "def deleta_test(base, n_amostras):\n",
        "    for i in range(n_amostras):\n",
        "        base = np.delete(base,i, 0)\n",
        "    \n",
        "    return base;\n",
        "\n",
        "def funcao_ativacao(mu):\n",
        "    if mu>=0:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "    \n",
        "def neuronio(x,w):\n",
        "    mu = np.dot(x.T,w)\n",
        "    y_hat = funcao_ativacao(mu)\n",
        "    return y_hat\n",
        "\n",
        "def perceptron(dados, eta=0.1, max_iter = 2000, erro_tol = 0.001):\n",
        "    n = dados.shape[0]  #numero de linhas\n",
        "    d = dados.shape[1] #numero de colunas\n",
        "    x = np.concatenate((np.ones((n,1)), dados[:,0:-1]), axis=1)\n",
        "    y = dados[:,-1]  #view altera no local de origem\n",
        "    \n",
        "    w = np.random.randn(d)\n",
        "    \n",
        "    it = 0\n",
        "    erro_total = erro_tol + 10;\n",
        "    \n",
        "    while((it<max_iter) and (erro_total> erro_tol)):\n",
        "        erro_total = 0\n",
        "        for i in range(n):\n",
        "            #calcular saida do neuronio\n",
        "            y_hat = neuronio(x[i,:],w)\n",
        "            #verificar o erro\n",
        "            erro = y[i] - y_hat\n",
        "            #atualizar os pesos\n",
        "            dw = eta*erro*x[i,:]\n",
        "            w += dw\n",
        "            erro_total += np.abs(erro)\n",
        "            \n",
        "        it +=1\n",
        "    \n",
        "    return w, erro_total, it\n",
        "\n",
        "def normalizacao(dados):\n",
        "    \n",
        "    norma = dados/dados.max(axis=0)\n",
        "    \n",
        "    return norma\n",
        "    \n",
        "\n",
        "def previsao_perceptron( teste, w):\n",
        "    resultado=np.zeros((teste.shape[0]))\n",
        "    \n",
        "    x = np.concatenate((np.ones((teste.shape[0],1)), teste[:,0:-1]), axis=1)\n",
        "    for i in range(teste.shape[0]):\n",
        "            #calcular saida do neuronio\n",
        "            y_hat = neuronio(x[i,:],w)\n",
        "            resultado[i] = y_hat\n",
        "    \n",
        "    return resultado\n",
        "\n",
        "def compara_resultado(resultado, teste):\n",
        "    acertos = 0\n",
        "    erros = 0\n",
        "    for i in range(teste.shape[0]):\n",
        "        if(resultado[i] == (teste[i,-1])):\n",
        "            acertos +=1\n",
        "        else:\n",
        "            erros+=1\n",
        "    \n",
        "    return erros, acertos\n",
        "       \n",
        "\n",
        "def main():\n",
        "    \n",
        "    base = cria_base_dados()\n",
        "    base[base[:,-1] == 2, -1] = -1 \n",
        "    \n",
        "    base = normalizacao(base)\n",
        "    \n",
        "    n_amostras = int((base.shape[0])*0.2)\n",
        "    coluna = int((base.shape[1]))\n",
        "    \n",
        "    conj_teste = cria_conj_teste(base, n_amostras, coluna)\n",
        "    \n",
        "    base = deleta_test(base, n_amostras)\n",
        "    \n",
        "    w, erro, iteracoes = perceptron(base, eta=0.7, max_iter = 1000, erro_tol = 0.001)\n",
        "    \n",
        "    resultado = previsao_perceptron(conj_teste, w)\n",
        "    \n",
        "    erros, acertos = compara_resultado(resultado, conj_teste)\n",
        "    \n",
        "\n",
        "    perc_acertos = acertos*100/(len(conj_teste))\n",
        "    perc_erros = erros*100/(len(conj_teste))\n",
        "    print(\"--------------------------\")\n",
        "    print(\"Resultados classificação perceptron\")\n",
        "    print(\"\")\n",
        "    print(\"--------------------------\\n\")\n",
        "    print(f\"Acertos:{acertos} de um total de {len(conj_teste)} amostras \\nEquivale a {perc_acertos}%\")\n",
        "    print(f\"\\nErros:{erros} de um total de {len(conj_teste)} amostras \\nEquivale a {perc_erros}%\")\n",
        "\n",
        "    print(\"--------------------------\")\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGDy0kRqpYaJ",
        "outputId": "e71b20a5-a8dc-4076-85fa-6461ef1a5233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------\n",
            "Resultados classificação perceptron\n",
            "\n",
            "--------------------------\n",
            "\n",
            "Acertos:149 de um total de 200 amostras \n",
            "Equivale a 74.5%\n",
            "\n",
            "Erros:51 de um total de 200 amostras \n",
            "Equivale a 25.5%\n",
            "--------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Criando Perceptron\n",
        "---\n",
        "Com conceitos de orientação a objetos"
      ],
      "metadata": {
        "id": "1kOqo9v6cR3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, data, learning_rate=0.7, n_iterations=2000)-> None:\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.dataset = data\n",
        "        self.n, self.d = np.shape(self.dataset)\n",
        "\n",
        "    def cria_conj_test(self) -> None:\n",
        "      self.n_amostras = int((self.dataset.shape[0])*0.2)\n",
        "      coluna = int((self.dataset.shape[1]))\n",
        "      np.random.shuffle(self.dataset)\n",
        "      self.conjunto_teste = np.zeros((self.n_amostras, coluna))\n",
        "\n",
        "      for i in range(self.n_amostras):\n",
        "          for j in range(coluna):\n",
        "              self.conjunto_teste[i,j] = self.dataset[i,j]\n",
        "      self.deleta_teste()\n",
        "\n",
        "    \n",
        "    def deleta_teste(self) ->None:\n",
        "      for i in range(self.n_amostras):\n",
        "        self.dataset = np.delete(self.dataset,i, 0)\n",
        "\n",
        "\n",
        "    def normaliza(self) -> None:\n",
        "      nInput = self.d - 1\n",
        "      for l in range(nInput):\n",
        "        self.dataset[:,l] = 2*(self.dataset[:,l] - self.dataset[:,l].min(axis=0)) / (self.dataset[:,l].max(axis=0) - self.dataset[:,l].min(axis=0)) - 1\n",
        "\n",
        "    def fit(self)-> None:\n",
        "        X = self.dataset[:,:-1]\n",
        "        X[np.isnan(X)] = 0\n",
        "        y = self.dataset[:,-1]\n",
        "        n_samples, n_features = self.dataset[:,:-1].shape\n",
        "\n",
        "\n",
        "        # Inicialize os pesos e o bias com zeros\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Treine o modelo por um número fixo de iterações\n",
        "        for i in range(self.n_iterations):\n",
        "            for j in range(n_samples):\n",
        "                # Calcule a saída do modelo\n",
        "                output = self.predict(X[j])\n",
        "\n",
        "                # Atualize os pesos e o bias usando a regra do Perceptron\n",
        "                try:              \n",
        "                  self.weights += self.learning_rate * (y[j] - output) * X[j]\n",
        "                  self.bias += self.learning_rate * (y[j] - output)\n",
        "                except:\n",
        "                  self.weights += 0.000000001\n",
        "                  self.bias += 0.0000000001\n",
        "    \n",
        "    def activation(self, x)-> int:\n",
        "        # Use a função de ativação degrau\n",
        "        return 1 if x >= 0 else -1\n",
        "\n",
        "    def predict(self, X)-> int:\n",
        "\n",
        "        # Calcule a saída do modelo\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self.activation(linear_output)\n",
        "\n",
        "        return y_predicted\n",
        "\n",
        "    def result(self)-> None:\n",
        "        self.acertos =0\n",
        "        self.erros =0\n",
        "        # Calcula resultado conjunto teste\n",
        "        for i in range(self.conjunto_teste.shape[0]):\n",
        "          y_predicted = self.predict(self.conjunto_teste[i,:-1])\n",
        "\n",
        "          if(y_predicted == self.conjunto_teste[i,-1]):\n",
        "            self.acertos+=1\n",
        "          else:\n",
        "            self.erros+=1\n",
        "\n",
        "        self.por_acertos = self.acertos*100/(len(self.conjunto_teste))\n",
        "        self.por_erros = self.erros*100/(len(self.conjunto_teste))\n",
        "        print(\"--------------------------\")\n",
        "        print(\"Resultados classificação Perceptron\")\n",
        "        print(\"\")\n",
        "        print(\"--------------------------\\n\")\n",
        "        print(f\"Acertos:{self.acertos} de um total de {len(self.conjunto_teste)} amostras \\nequivale a {self.por_acertos}%\")\n",
        "        print(f\"Erros:{self.erros} de um total de {len(self.conjunto_teste)} amostras \\nequivale a {self.por_erros}%\")\n",
        "\n",
        "        print(\"--------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "  dataset = np.genfromtxt('base.txt')\n",
        "  dataset[dataset[:,-1] == 2, -1] = -1  \n",
        "\n",
        "  model = Perceptron(dataset)\n",
        "  model.normaliza()\n",
        "  model.cria_conj_test()\n",
        "  model.fit()\n",
        "  model.result()\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEIQSo5L69rP",
        "outputId": "48dc2a32-afaf-4148-b6bf-a7817e5f83a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------\n",
            "Resultados classificação Perceptron\n",
            "\n",
            "--------------------------\n",
            "\n",
            "Acertos:142 de um total de 200 amostras \n",
            "equivale a 71.0%\n",
            "Erros:58 de um total de 200 amostras \n",
            "equivale a 29.0%\n",
            "--------------------------\n"
          ]
        }
      ]
    }
  ]
}